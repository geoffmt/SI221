# -*- coding: utf-8 -*-
"""K-nearestNeighbors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qM_5BG44QRLsnN3uXdpJh_FzvrQUnq6H

# **SI221 Practical assigment 2 : k-Nearest Neighbors**

**1- k-NN calssification : Synthetic dataset**

Question 1:
"""

from random import *
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from scipy.spatial import distance
from math import sqrt
from matplotlib.colors import ListedColormap
from matplotlib import rc
import seaborn as sns


#k-NN classification algorithm for one sample
def knn_classification(x, X_train, y_train, k):
  L = []
  for i in range(X_train.shape[0]) : 
    L.append([distance.euclidean(x, X_train[i]),i])
  L.sort(key = lambda x: x[0])
  indices = []
  for i in range(k):
    indices.append(L[i][1])
  s0 = s1 = s2 = 0
  for i in indices:
    if (y_train[i] == 0) :
      s0 += 1
    if (y_train[i] == 1) :
      s1 += 1
    if (y_train[i] == 2) :
      s2 += 1
  return np.argmax([s0,s1,s2])


#Generate the dataset
X, y = make_blobs(n_samples=300, centers=[[-1,0],[1,0],[0,1]], n_features=2,cluster_std= sqrt(0.1))

#Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=2)

K = [1,2,5,10]

for j in K:
    min_tot = np.min(X)
    max_tot = np.max(X)
    delta = (max_tot - min_tot) / 50
    xx, yy = np.meshgrid(np.arange(min_tot, max_tot, delta),
                         np.arange(min_tot, max_tot, delta))
    z = np.array([knn_classification(vec, X_train, y_train, j) for vec in np.c_[xx.ravel(), yy.ravel()]])
    z = z.reshape(xx.shape)
    labels = np.unique(z)
    color_blind_list = sns.color_palette("colorblind", labels.shape[0])
    sns.set_palette(color_blind_list)
    my_cmap = ListedColormap(color_blind_list)
    plt.imshow(z, origin='lower', extent=[min_tot, max_tot, min_tot, max_tot],
               interpolation="mitchell", alpha=0.80, cmap=my_cmap)

    ax = plt.gca()
    cbar = plt.colorbar(ticks=labels)
    cbar.ax.set_yticklabels(labels)

    k = np.unique(y).shape[0]
    color_blind_list = sns.color_palette("colorblind", k)
    symlist = ['o', 'p', '*', 's', '+', 'x', 'D', 'v', '-', '^']
    for i, label in enumerate(y):
        plt.scatter(X[i, 0], X[i, 1], c=[color_blind_list[int(y[i])]],
                    s=80, marker=symlist[int(label)])
    plt.ylim([min_tot, max_tot])
    plt.xlim([min_tot, max_tot])
    ax.get_yaxis().set_ticks([])
    ax.get_xaxis().set_ticks([])
    plt.title('K = '+ str(j))
    plt.show()

"""**For a small value of K, the decision boundary is more jagged which means we have a low bias but a high variance. As we increase the value of K, the decision boundary is smoother and we have less emphasis on individual points, which means lower variance but increased bias.**

Question 2 :
"""

#Function to compute the average error and its standard deviation over 50 
##randomly generated datasets

def average_error_and_standard_deviation(stdcarre, k):
  L = []

  for i in range (50):
    X, y = make_blobs(n_samples=300, centers=[[-1,0],[1,0],[0,1]], n_features=2, cluster_std=sqrt(stdcarre))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=2)
    s = 0
    for j in range(X_test.shape[0]):
      if y_test[j] != knn_classification(X_test[j], X_train, y_train, k) :
        s += 1
    L.append(s)

  return [np.mean(L),np.std(L)]

#Let's fix K and the variance
[average_error,standard_deviation] = average_error_and_standard_deviation(0.05, 5)
print ("The average error is : ", average_error)
print ("Its standard deviation is : ", standard_deviation)


#Let's fix K and plot for different values of the variance
abscisse = [0.05, 0.10, 0.15, 0.20, 0.25]
labels = ['0.05', '0.10', '0.15', '0.20', '0.25']

fig = plt.figure()
fig.subplots_adjust(hspace=0.4, wspace=0.4)
s=1

for k in [1, 2, 5, 10]:
  x_pos = np.arange(len(abscisse))
  CTEs = [average_error_and_standard_deviation(i,k)[0] for i in abscisse]
  error = [average_error_and_standard_deviation(i,k)[1] for i in abscisse]
  ax = fig.add_subplot(2, 2, s)
  ax.bar(x_pos, CTEs, yerr=error, align='center', alpha=0.5, ecolor='black', capsize=10)
  ax.set_ylabel('Average error')
  ax.set_xticks(x_pos)
  ax.set_xticklabels(labels)
  ax.set_title("e($\sigma$,k="+str(k)+") ")
  ax.yaxis.grid(True)
  s += 1

plt.tight_layout()
plt.show()


#Let's fix the variance and plot for different values of K
abscisse = [1, 2, 5, 10]
labels = ['1', '2', '5','10']

fig = plt.figure()
fig.subplots_adjust(hspace=0.4, wspace=0.4)
p=1

for sig in [0.10, 0.15, 0.20, 0.25]:
  x_pos = np.arange(len(abscisse))
  CTEs = [average_error_and_standard_deviation(sig,i)[0] for i in abscisse]
  error = [average_error_and_standard_deviation(sig,i)[1] for i in abscisse]
  ax = fig.add_subplot(2, 2, p)
  ax.bar(x_pos, CTEs, yerr=error, align='center', alpha=0.5, ecolor='black', capsize=10)
  ax.set_ylabel('Average error')
  ax.set_xticks(x_pos)
  ax.set_xticklabels(labels)
  ax.set_title("e($\sigma$ = "+str(sig)+",k) ")
  ax.yaxis.grid(True)
  p += 1

plt.tight_layout()
plt.show()

"""**2- k-NN regression: Szeged-weather dataset**

Question 1 :
"""

import pandas as pd

#Let's import the data 
data = pd.read_csv("weatherHistory.csv")

temperature = data['Temperature (C)']
humidity = data['Humidity']
app_temperature = data['Apparent Temperature (C)']

#Let's plot
plt.scatter(temperature, humidity, c=app_temperature)
plt.title('Visualization of the dataset in terms of temperature, humidity, and apparent temperature')
plt.xlabel('Temperature')
plt.ylabel('Humidity')
plt.colorbar()
plt.show(block = False)

"""Question 2 :"""

#k-NN regression algorithm
def knn_regression(x, X_train, y_train, k):
  L = []
  for i in range(X_train.shape[0]) : 
    L.append([distance.euclidean(x, X_train[i]),i])
  L.sort(key = lambda x: x[0])
  indices = []
  for i in range(k):
    indices.append(L[i][1])
  return np.mean(y_train[indices])

from sklearn.model_selection import KFold

#We consider the first 2000 samples of the dataset 
temperature = temperature[:2000]
humidity = humidity[:2000]
app_temperature = app_temperature[:2000]


#Function to compute the average error and its standard deviation for a K value
def average_error_and_standard_deviation_regression(k) :
  M = []
  #We repeat 5 five times the following
  for h in range(5):
    #We permute the order of the samples at random
    indices = np.arange(2000)
    np.random.shuffle(indices)

    temperature_new = np.array(temperature[indices])
    humidity_new = np.array(humidity[indices])
    app_temperature_new = np.array(app_temperature[indices])

    X = np.concatenate((temperature_new.reshape(-1,1), humidity_new.reshape(-1,1)), axis=1)
    y = app_temperature_new.reshape(-1,1)

    L = []
    kf = KFold(n_splits=5)
    for train_index, test_index in kf.split(X):
      X_train, X_test = X[train_index], X[test_index]
      y_train, y_test = y[train_index], y[test_index]
      y_predict = np.array([knn_regression(X_test[i], X_train, y_train, k) for i in range(X_test.shape[0])])
      L.append(sum((y_test-y_predict)**2))

    M.append(np.mean(L))

  return [np.mean(M), np.std(M)]


#Let's plot for different values of K

abscisse = [1,2,3,4,5,6,7,8,9,10]
x_pos = np.arange(len(abscisse))
labels = ['1','2','3','4','5','6','7','8','9','10']

CTEs = []
error = []
for i in abscisse :
  average, std = average_error_and_standard_deviation_regression(i)
  CTEs.append(average)
  error.append(std)

fig, ax = plt.subplots()
ax.bar(x_pos, CTEs, yerr=error, align='center', alpha=0.5, ecolor='black', capsize=10)
ax.set_ylabel('Average error')
ax.set_xticks(x_pos)
ax.set_xticklabels(labels)
ax.set_title("e(k)")
ax.yaxis.grid(True)

plt.tight_layout()
plt.show()

"""**3- k-NN classification: MNIST dataset**

Question 1:
"""

import scipy.io
train = scipy.io.loadmat('data_app.mat') 
test = scipy.io.loadmat('data_test.mat')

digits_train = train.get("x")
digits_test = test.get("x")
labels_train = train.get("S")
labels_test = test.get("S")

#We convert and normalize the images
digits_train = digits_train.astype(float) / 255
digits_test = digits_test.astype(float) / 255

#We visualize a part the dataset
fig = plt.figure()
fig.subplots_adjust(hspace=0.4, wspace=0.4)
p=1

for i in range(25):
  image = digits_train[i].reshape((28,28))
  ax = fig.add_subplot(5, 5, p)
  ax.imshow(image)
  p += 1

plt.show()

#Let's plot the histograms of the label

plt.hist(labels_test)
plt.title('Labels of the test set', fontsize=10)
plt.show()

plt.hist(labels_train)
plt.title('Labels of the train set', fontsize=10)
plt.show()

"""Question 2:"""

# A function that classifies the test dataset
def knn(digits_test, digits_train, labels_train, k):
  labels_predict = [] 
  for x in digits_test :
    L = []
    for i in range(X_train.shape[0]) : 
      L.append([distance.euclidean(x, digits_train[i]),i])
    L.sort(key = lambda x: x[0])
    indices = []
    for i in range(k):
      indices.append(L[i][1])
    s = np.zeros((10,1))
    for i in indices:
      if labels_train[i] == 10 :
        s[0] += 1
      else : 
        s[labels_train[i]] += 1
    if np.argmax(s) == 0 :
      labels_predict.append(10)
    else :
      labels_predict.append(np.argmax(s))
  return np.array(labels_predict).reshape(-1,1)

#Function that computes the error rate :
def error_rate(labels_test, labels_predict):
  error = 0
  for i in range(labels_test.shape[0]):
    if labels_test[i] != labels_predict[i] :
      error += 1
  return error/labels_test.shape[0]


#Let's compute the error rate for different values of K
for k in [1,3,5]:
  labels_predict = knn(digits_test, digits_train, labels_train, k)
  error = error_rate(labels_test, labels_predict)
  print ('The error rate for k =', k, ' is :',error)

"""Question 3:"""

import itertools

#Function that plots the confusion matrix
def confusion_matrix(labels_test, labels_predict):
  matrix = np.zeros((10,10))
  for i in range(labels_test.shape[0]):
    if labels_test[i] == 10 and labels_predict[i] == 10 :
      matrix[0,0] += 1
    elif labels_test[i] == 10 and labels_predict[i] != 10 :
      matrix[0,labels_predict[i]] += 1
    elif labels_test[i] != 10 and labels_predict[i] == 10 :
      matrix[labels_test[i],0] += 1
    else :
      matrix[labels_test[i],labels_predict[i]] += 1
  return matrix

#Function to plot the confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

#For K = 3
labels_predict = knn(digits_test, digits_train, labels_train, 3)
matrix = confusion_matrix(labels_test, labels_predict)
plot_confusion_matrix(cm = matrix, 
                      normalize = False,
                      classes = ['0','1','2','3','4','5','6','7','8','9'])